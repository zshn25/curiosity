{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers fusion for faster neural network inference\n",
    "\n",
    "> How to fuse all convolution and batch normalization layers in a Pytorch model during test time for faster inference\n",
    "\n",
    "- image: images/neural-network-3816319.svg\n",
    "- date:   2021-04-27 15:21:23 -0700\n",
    "- categories: deep-learning pytorch fast-inference\n",
    "- author: Zeeshan Khan Suri\n",
    "- published: true\n",
    "- comments: true\n",
    "- badges: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [previous post]({% post_url 2021-03-11-is-convolution-linear %}), we proved that convolutions are linear. There are other linear layers in a neural network such as a batch normalization layer. A batch normalization layer [normalizes](https://en.wikipedia.org/wiki/Normalization_(statistics) its input batch to have zero mean and unit standard deviation, which are calculated from the input batch.{% fn 1 %} It basically translates/shifts and scales the input batch, thus being a linear operation. In many network architectures such as ResNets{% fn 2 %} and DenseNets{% fn 3 %}, a convolutional layer followed by a batch norm layer is used. \n",
    "\n",
    "During training, the mean and standard deviation of the input batch are used in the batch normalization and are eventually learnt. During inference, these estimates of mean and standard deviation are used instead. The idea of this post is to fuse these two consecutive layers during inference, thereby reducing computation and thus inference time.\n",
    "\n",
    "Note that this must not be done during training since the input batch's mean and standard deviation are not yet learnt and fusing before training will be same as removing the batch normalization completely.\n",
    "\n",
    "Pytorch provides a utility [function to fuse convolution and batch norm](https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/fusion.py), although this was meant for the use of quantization. In this post, I share the following function to recursively check and fuse all consecutive convolution and batch norm layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.fusion import fuse_conv_bn_eval\n",
    "\n",
    "def fuse_all_conv_bn(model):\n",
    "    \"\"\"\n",
    "    Fuses all consecutive Conv2d and BatchNorm2d layers.\n",
    "    License: Copyright Zeeshan Khan Suri, CC BY-NC 4.0\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for name, module in model.named_children(): # immediate children\n",
    "        if list(module.named_children()): # is not empty (not a leaf)\n",
    "            fuse_all_conv_bn(module)\n",
    "            \n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            if isinstance(stack[-1][1], nn.Conv2d):\n",
    "                setattr(model, stack[-1][0], fuse_conv_bn_eval(stack[-1][1], module))\n",
    "                setattr(model, name, nn.Identity())\n",
    "        else:\n",
    "            stack.append((name, module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Fusing all convolution and batch norm layers of ResNet101 makes the resulting model **~25% faster** with negligible difference in the model's output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time before fusion:\n",
      "43.4 ms ± 17.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Inference time after fusion:\n",
      "31.2 ms ± 7.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Error between outputs before and after fusion:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.5947e-05, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "import torch\n",
    "from torchvision.models.resnet import resnet101\n",
    "\n",
    "model=resnet101(pretrained=True).to('cuda')\n",
    "model.eval()\n",
    "rand_input = torch.randn((1,3,256,256)).to('cuda')\n",
    "\n",
    "# Forward pass\n",
    "output = model(rand_input)\n",
    "print(\"Inference time before fusion:\")\n",
    "%timeit model(rand_input)\n",
    "\n",
    "# Fuse Conv BN\n",
    "fuse_all_conv_bn(model)\n",
    "print(\"\\nInference time after fusion:\")\n",
    "%timeit model(rand_input)\n",
    "# compare result\n",
    "print(\"\\nError between outputs before and after fusion:\")\n",
    "torch.norm(torch.abs(output - model(rand_input))).data"
   ]
  },
  {
   "source": [
    "Note that the same can be done with any network with 2 or more consecutive linear layers to reduce inference time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Zeeshan Khan Suri, [<i class=\"fab fa-creative-commons\"></i> <i class=\"fab fa-creative-commons-by\"></i> <i class=\"fab fa-creative-commons-nc\"></i>](http://creativecommons.org/licenses/by-nc/4.0/)\n",
    "\n",
    "If this article was helpful to you, consider citing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@misc{suri_Layers-fusion-for-faster-inference_2021,\n",
    "      title={Layers fusion for faster neural network inference},\n",
    "      url={https://zshn25.github.io/Layers-fusion-for-faster-inference/}, \n",
    "      journal={Curiosity}, \n",
    "      author={Suri, Zeeshan Khan}, \n",
    "      year={2021}, \n",
    "      month={Apr}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "{{ 'Ioffe, S. & Szegedy, C.. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:448-456 [Link](http://proceedings.mlr.press/v37/ioffe15.html).' | fndetail: 1 }}\n",
    "{{ 'K. He, X. Zhang, S. Ren and J. Sun, \"Deep Residual Learning for Image Recognition,\" 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778, doi: [10.1109/CVPR.2016.90](https://doi.org/10.1109/CVPR.2016.90). ' | fndetail: 2 }}\n",
    "{{ 'G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger, \"Densely Connected Convolutional Networks,\" 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2261-2269, doi: [10.1109/CVPR.2017.243](https://doi.org/10.1109/CVPR.2017.243). ' | fndetail: 3 }}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}